# Détection d'Anomalies

Dans la continuation de notre travail avec Kimura, nous allons maintenant passer au data stream anomaly detection problem.

Il y a en gros 3 techniques pour a détection d’anomalies, qui sont parfaitement résumées dans le papier suivant qui attaque la question du « online » (comme nous pour le clustering) : https://hal.archives-ouvertes.fr/hal-02874869v2/file/IForestASD_ScikitMultiflow_Version2_PourHAL.pdf

Il faudrait faire une implémentation en C de ce qui est proposé dans cet article. Sans doute en réduisant la dimension des données en entrée qui pourraient être des données 2D comme on a fait pour le clustering.

Notre fond de commerce c’est de pouvoir exécuter sur des devices comme celui de Terasic mentionné ci-après. Tu vas reconnaître des SDK pour ce type de board.

L’article sur HAL, lui, suppose que nous travaillons dans l’écosystème Python (voir la communauté Tinyml (https://www.tinyml.org/) , voir micropython (https://micropython.org/), voir river cité dans l’article https://github.com/online-ml/river/ et https://scikit-multiflow.github.io/). C’est la vision un peu orthogonale à la notre. Nous n’allons pas aller dans cette direction.

# Détails de l'Article Mariam Barry & al.

1. iForestASD_Streaming_Scikit-MultiFlow

2. skmultiflow_IForestASD

3. streaming-online-learning

## iForestASD_Streaming_Scikit-MultiFlow.py

        # -*- coding: utf-8 -*-
      """Notebook_test_iForestASD_Scikitmultiflow.ipynb

      http://localhost:8888/edit/cerin24022022/py-mariambarry/iForestASD_Streaming_Scikit-MultiFlow/Notebook_24032020_2325/iforestasd_scikitmultiflow.py

      Automatically generated by Colaboratory.

      Original file is located at
          https://colab.research.google.com/drive/1APZBgZ0fuHufYWM5QKqFhR7cpPs0bF2T

      # iForestASD :  Unsupervised Anomaly Detection with Scikit-MultiFlow

      An Implementation of Unsupervised Anomaly Detection with Isolation Forest in Scikit-MultiFlow with Sliding Windows \& drift detection


      ## References :

       - An Anomaly Detection Approach Based on Isolation Forest  for Streaming Data using Sliding Window (Ding \& Fei, 2013) https://www.sciencedirect.com/science/article/pii/S1474667016314999

       - Isolation-based Anomaly Detection (Liu, Ting \& Zhou, 2011) https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/tkdd11.pdf

       - Scikit MultiFlow HalfSpace Trees Implementation - “Fast anomaly detection for streaming data,” in IJCAI Proceedings - S.C.Tan, K.M.Ting, and T.F.Liu, 
       https://scikit-multiflow.github.io/scikit-multiflow/_autosummary/skmultiflow.anomaly_detection.HalfSpaceTrees.html#id2

       - Original implementation of Isolation Forest (not the one in SK-learn) https://github.com/Divya-Bhargavi/isolation-forest
      """



      """# Notebook File Structure  is the following

      Part 1 - Main Class contains
        - Init,
        - Partial_fit,
        - Update_model,
        - Predict methods which use the anomaly_score methods of the iForest class

      Part 2 - Isolation Forest class (re-used) and main functions
       - 

      Part 3 - Testing some examples and comparison of HS-Trees and IsolatationForestStream 
      - on synthetic 
      - on Real (HTTP) data.

      ## Import lib and packages
      """



      """## Install Cyphion then load the Scikit-multiflow latest release from Github"""

      ## !pip install scikit-multiflow



      #!pip install -U git+https://github.com/scikit-multiflow/scikit-multiflow

      from skmultiflow.core import BaseSKMObject, ClassifierMixin

      from skmultiflow.utils import check_random_state

      import numpy as np

      import random


      """# Part 1 - Main class - IsolationForestStream"""

      ## To implement this class, we took inspiration from Scikit-MultiFLow HSTrees implementation to follow its requirements.

      class IsolationForestStream(BaseSKMObject, ClassifierMixin):

        """
        This code implements  Anomaly Detection Approach Based on Isolation Forest Algorithm for Streaming Data Using Sliding Window (Ding \& Fei, 2013) [3]

          Each sample has an anomaly score is computed based on Isolation Forest anomaly based approach [2]. The concept of Isolation forest [1]
          consists on  isolating observations by randomly selecting a feature
          and then randomly selecting a split value between the maximum and minimum
          values of the selected feature.

          Model is updated of a Drift has been detected based on a input drift threshold. The drift detection approach is proposed by [2] 
          and works as follow : if the averaged anomaly score between two successive sliding windows is highter than the drift threshold (u), 
          then the previous model is completely discarded and a new model is build as an isolation forest on latest sliding windows stream.


        Parameters

          ---------

          n_estimators: int, optional (default=25)

             Number of trees in the ensemble.

             't' in the original paper.



          window_size: int, optional (default=100)

              The window size of the stream.

              ψ, 'Psi' in the original paper.   

      ## Optional       

          anomaly_threshold: double, optional (default=0.5)

              The threshold for declaring anomalies.

              Any instance prediction probability above this threshold will be declared as an anomaly.

          drift_threshold: double, optional (default=0.5)

              The threshold for detecting Drift and update the model.

             If the averaged anomaly score between two successive sliding windows is highter than the threshold (u), 
          then the previous model is completely discarded and a new model is build as an isolation forest on latest sliding windows stream.
          This parameters is supposed to be know by an expert domain, depending on data set.

      ## Other Attributes

          ensemble : Isolation Tree Ensemble

              Contain an Isolation Tree Ensemble object, current model for   IsolationForestStream

          sample_size : int

              Number of sample seen since the update

          anomaly_rate : float

              Rate of the anomalies in the previous sliding window (AnomalyRate in the original paper iForestASD)

          prec_window & window : numpy.ndarray of shape (n_samples, self.window_size)

              The previous and current window of data

          cpt : int

              Counter, if the n_estimator is higher than its, it will fit

          References
          ----------

          [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua.        
      “Isolation forest.” Data Mining, 2008. ICDM’08. Eighth IEEE International Conference on.

          [2] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation-based anomaly detection.” ACM Transactions on Knowledge Discovery from Data (TKDD) 6.1 (2012): 
      self.n_estimators

          [3] Ding, Zhiguo. (2013) An Anomaly Detection Approach Based on Isolation Forest Algorithm for Streaming Data Using Sliding Window. 12-17. 10.3182/20130902-3-CN-3020.00044. 

          """ 

        def __init__(self, window_size=100, n_estimators=25, anomaly_threshold=0.5, drift_threshold=0.5, random_state=None):

              super().__init__()

              self.n_estimators = n_estimators

              self.ensemble = None

              self.random_state = random_state

              self.window_size = window_size

              self.samples_seen = 0

              self.anomaly_rate = 0.20 

              self.anomaly_threshold = anomaly_threshold

              self.drift_threshold = drift_threshold

              self.window = None

              self.prec_window = None

              self.cpt = 0


        def partial_fit(self, X, y, classes=None, sample_weight=None):

                """ Partially (incrementally) fit the model.
                Parameters
                ----------
                X : numpy.ndarray of shape (n_samples, n_features)
                    The features to train the model.
                y: numpy.ndarray of shape (n_samples)
                    An array-like with the class labels of all samples in X.
                classes: None
                    Not used by this method.
                sample_weight: None
                    Not used by this method.
                Returns
                -------
                self
                """

                ## get the number of observations
                number_instances, _ = X.shape

                if(self.samples_seen==0):
                  ## ToDo ? Give a sample of self.window_size in attribute of iForest
                  iforest = IsolationTreeEnsemble(self.window_size,self.n_estimators,self.random_state)
                  self.ensemble = iforest


                for i in range(number_instances):
                    self._partial_fit(X[i], y[i])

                return self


        def _partial_fit(self, X, y):

                """ Trains the model on samples X and corresponding targets y.
                Private function where actual training is carried on.
                Parameters
                ----------
                X: numpy.ndarray of shape (1, n_features)
                    Instance attributes.
                y: int
                    Class label for sample X. Not used in this implementaion which is Unsupervised
                """ 

                """
                Reshape X and add it to our window if it isn't full.
                If it's full, give window to our precedent_window.
                If we are at the end our window, fit if we're learning 
                Check the anomaly score of our window 
                Update if self.anomaly_rate > self.drift_threshold

                """
                X = np.reshape(X,(1,len(X)))

                if self.samples_seen % self.window_size == 0:
                  ## Update the two windows (precedent one and current windows)
                  self.prec_window = self.window
                  self.window = X
                else:
                  self.window = np.concatenate((self.window,X))


                if self.samples_seen % self.window_size == 0 and self.samples_seen !=0:
                    #Fit the ensemble if it's not empty
                    if(self.cpt<self.n_estimators):
                      self.ensemble.fit(self.prec_window)
                      self.cpt += 1                  
                        ## Update the current anomaly score
                    self.anomaly_rate = self.anomaly_scores_rate(self.prec_window) ## Anomaly rate
                    #print(self.anomaly_rate) ## 

                        ## Update the model if the anomaly rate is greater than the threshold (u in the original paper [3])
                    if self.anomaly_rate > self.drift_threshold: ## Use Anomaly RATE ?
                      self.update_model(self.prec_window) # This function will discard completly the old model and create a new one

                self.samples_seen += 1

        def update_model(self,window):
          """ Update the model (fit a new isolation forest) if the current anomaly rate (in the previous sliding window)
           is higher than self.drift_threshold
              Parameters: 
                window: numpy.ndarray of shape (self.window_size, n_features)
              Re-Initialize our attributes and our ensemble, fit with the current window

          """

          ## ToDo ? Give a sample of self.window_size in attribute of iForest
          self.is_learning_phase_on = True
          iforest = IsolationTreeEnsemble(self.window_size,self.n_estimators,self.random_state)
          self.ensemble = iforest
          self.ensemble.fit(window)
          print("iForest ASD Update the model by training a new iForest")


        def anomaly_scores_rate(self, window):
          """
          Given a 2D matrix of observations, compute the anomaly rate 
          for all instances in the window and return an anomaly rate of the given window.

          Parameters :
          window: numpy.ndarray of shape (self.window_size, n_features)
          """

          score_tab = 2.0 ** (-1.0 * self.ensemble.path_length(window) / c(len(window)))
          score = 0
          for x in score_tab:
            if x > self.anomaly_threshold:
              score += 1
          return score / len(score_tab)

        def predict(self, X):
          """
          Given an instance, Predict the anomaly (1 or 0) based on the last sample of the window by using predict_proba if our model have fit, 
          else return None

          """
          if(self.samples_seen <= self.window_size):

            return [-1] ## Return the last element

          X = np.reshape(X,(1,len(X[0])))
          self.prec_window = np.concatenate((self.prec_window ,X)) ## Append the instances in the sliding window

          prediction =  self.ensemble.predict_from_anomaly_scores(self.predict_proba(self.prec_window),self.anomaly_threshold) ## return 0 or 1

          return [prediction]

        def predict_proba(self, X):
          """
          Calculate the anomaly score of the window if our model have fit, else return None
          Parameters :
          X: numpy.ndarray of shape (self.window_size, n_features)   

          """
          if(self.samples_seen <= self.window_size):
              return [-1]
          return self.ensemble.anomaly_score(self.prec_window)[-1] # Anomaly return an array with all scores of each data, taking -1 return the last instance (X) anomaly score

      """# Part 2- IsolationTreeEnsemble  Class (iForest in the original paper)"""

      # Follows original paper algo from https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf
      # Original Source re-used and adpted to our project from https://github.com/Divya-Bhargavi/isolation-forest 
      class IsolationTreeEnsemble:
          def __init__(self, sample_size, n_trees, random_state):
              self.sample_size = sample_size
              self.n_trees = n_trees
              self.depth = np.log2(sample_size)
              self.trees = []
              self.random_state = random_state
              self._random_state = check_random_state(self.random_state)
              self.is_learning_phase_on = True 

          def fit(self, X:np.ndarray):
              """
              Given a 2D matrix of observations, create an ensemble of IsolationTree
              objects and store them in a list: self.trees.  Convert DataFrames to
              ndarray objects.
              """
              len_x = len(X)

              for i in range(self.n_trees):
                  sample_idx = random.sample(list(range(len_x)), self.sample_size )
                  temp_tree = IsolationTree(self.depth, 0).fit(X[sample_idx])
                  self.trees.append(temp_tree)

              return self

          def path_length(self, X:np.ndarray):
              """
              Given a 2D matrix of observations, X, compute the average path length
              for each observation in X.  Compute the path length for x_i using every
              tree in self.trees then compute the average for each x_i.  Return an
              ndarray of shape (len(X),1).
              """
              pl_vector = []

              for x in (X):
                  pl = np.array([path_length_tree(x, t, 0) for t in self.trees])
                  pl = pl.mean()

                  pl_vector.append(pl)

              pl_vector = np.array(pl_vector).reshape(-1, 1)

              return pl_vector

          def anomaly_score(self, X:np.ndarray):
              """
              Given a 2D matrix of observations, X, compute the anomaly score
              for each x_i observation, returning an ndarray of them.
              """
              return 2.0 ** (-1.0 * self.path_length(X) / c(len(X)))

          def predict_from_anomaly_scores(self, scores:int, threshold:float):
              """
              Given an array of scores and a score threshold, return an array of
              the predictions: 1 for any score >= the threshold and 0 otherwise.
              """
              predictions = 1 if scores >= threshold else 0

              return predictions


      class IsolationTree:
          def __init__(self, height_limit, current_height):

              self.depth = height_limit
              self.current_height = current_height
              self.split_by = None
              self.split_value = None
              self.right = None
              self.left = None
              self.size = 0
              self.exnodes = 0
              self.n_nodes = 1

          def fit(self, X:np.ndarray):
              """
              Given a 2D matrix of observations, create an isolation tree. Set field
              self.root to the root of that tree and return it.
              If you are working on an improved algorithm, check parameter "improved"
              and switch to your new functionality else fall back on your original code.
              """

              if len(X) <= 1 or self.current_height >= self.depth:
                  self.exnodes = 1
                  self.size = X.shape[0]

                  return self

              split_by = random.choice(np.arange(X.shape[1]))
              X_col = X[:, split_by]
              min_x = X_col.min()
              max_x = X_col.max()

              if min_x == max_x:
                  self.exnodes = 1
                  self.size = len(X)

                  return self

              else:

                  split_value = min_x + random.betavariate(0.5, 0.5) * (max_x - min_x)

                  w = np.where(X_col < split_value, True, False)
                  del X_col

                  self.size = X.shape[0]
                  self.split_by = split_by
                  self.split_value = split_value

                  self.left = IsolationTree(self.depth, self.current_height + 1).fit(X[w])
                  self.right = IsolationTree(self.depth, self.current_height + 1).fit(X[~w])
                  self.n_nodes = self.left.n_nodes + self.right.n_nodes + 1

              return self

      def c(n):
          if n > 2:
              return 2.0*(np.log(n-1)+0.5772156649) - (2.0*(n-1.)/(n*1.0))
          elif n == 2:
              return 1
          if n == 1:
              return 0

      def path_length_tree(x, t,e):
          e = e
          if t.exnodes == 1:
              e = e+ c(t.size)
              return e
          else:
              a = t.split_by
              if x[a] < t.split_value :
                  return path_length_tree(x, t.left, e+1)
              if x[a] >= t.split_value :
                  return path_length_tree(x, t.right, e+1)

# Résultats de nos RECHERCHES

## 1. Concepts et notions

La détection d'anomalies est une branche du forage de données qui s'occupe de l'identification des enregistrements atypiques ou des occurrences rares dans les
données (Tan et al., 2006). En d'autres termes, la détection d'anomalies consiste à trouver les objets qui sont différents ou inconsistants par rapport à la majorité des objets d 'un jeu de données. Dans la littérature, les objets atypiques détectés sont dits anomalies, et sont aussi appelés, selon le contexte d'application, exceptions, surprises ou outliers (Aggarwal, 2017).

Initialement, la détection d'anomalies s'est développée dans les données à vecteur de caractéristiques. Formellement, la première définition d'anomalie revient à Hawkins en 1980 : "Une anomalie est une observation qui diffère tellement d'autres observations au point d'éveiller des soupçons qu'elle soit générée par un mécanisme différent" (Hawkins, 1980). 
Étant donnée la force d'expressivité des graphes et leur capacité à représenter des relations complexes entre les entités du monde réel, la notion d'anomalie s'est généralisée au cas des données représentées par des graphes. En effet, "dans un graphe, une anomalie peut être définie par les objets qui sont rares et qui diffèrent significativement de la majorité des objets de référence ." (Akoglu et al. , 2015). Ici , un objet de référence est un objet qui se caractérise par un comportement ou par un état normal attendu.

La définition d'anomalie prend plus de sens lorsqu'elle est reliée à un contexte ou à une application bien spécifique. Particulièrement , la détection d'anomalies a été beaucoup appliquée pour dans la détection de fraude. Par ailleurs, la détection d'anomalies s'emploie aussi dans la détection des intrusions dans les réseaux d'ordinateurs. 

## 2. Détection d'anomalies dans les données vectorielles

Le techniques de détection d'anomalies dan les données vectorielles peuvent être catégorisées en trois types : méthodes à base de statistiques, méthodes à base de distance et méthodes à base de densité. 

### 2.1 Méthodes à base de statistiques

Les approches à base de statistiques consistent à élaborer des modèles statistiques probabilistes flexibles qui représentent la distribution des jeux de données testés comme les modèles gaussiens (Yamanishi et al., 2004) et les modèles de régression (Aggarwal, 2005) , (Li et Han , 2007). Le degré d'anomalie d'un objet particulier est évalué par rapport à sa conformité au modèle établi. Particulièrement, dans (Yamanishi et al., 2004), un modèle de mélange gaussien est proposé pour représenter la distribution des données testées. Chaque objet reçoit un score d'anomalie qui caractérise sa déviation au modèle. Un score élevé dénote une forte probabilité que l'objet en question soit une anomalie.

### 2.2 Méthodes à base de distance

Les méthodes à base de distance consistent à calculer la disparité entre les objets d'un ensemble de données. Pour mesurer l'hétérogénéité des objets, plusieurs métriques peuvent être employées comme la distance euclidienne et la distance de Manhattan. Un objet est considéré une anomalie s'il est remarquablement distant de la majorité d'objets.
Spécifiquement les techniques à base de distance comme k plus proches voisins (Ramaswamy et al. , 2000) et KNN-pondéré (Angiulli et Pizzuti, 2002) assignent
un score d'anomalie à chaque objet en se basant sur ses k plus proches voisins. De cette manière, étant distants , les anomalies ( outliers) reçoivent des scores élevés et les objets normaux ( inliers) reçoivent des scores faibles. Les anomalies sont discernées en triant les scores dans un ordre ascendant et en sélectionnant les observations ayant les scores les moins élevés.

### 2.3 Méthodes à base de densité

Les méthodes à base de densité mesurent le degré d'anomalie d'un objet en considérant la densité locale de son voisinage. Spécifiquement, l'exemple de calcul du score d'anomalie LOF (Local Outlier Factor) (Breunig et al., 2000). Le fondement de LOF a été inspiré de la méthode de partitionnement à base de densité DBSCAN qui identifie à la fois les communautés et les outliers (Ester et al., 1996). Dans LOF, la densité locale de chaque objet se calcule en respect de
ses k plus proches voisins. L'ensemble des distances d'un objet particulier à ses k plus proches voisins sont utilisées dans le calcul de sa densité locale. Les densités locales de tous les objets sont, ensuite, évaluées pour déterminer les régions de densité similaires et les objets outliers qui détiennent des densités locales remarquablement faibles par rapport à leurs voisinages. 
Les techniques présentées dans cette section ne représentent pas une liste exhaustive des méthodes de détection d'anomalies dans les données vectorielles. 

## 3. Détection d'anomalies dans les graphes

Un grand intérêt a été porté à l'élaboration de techniques qui traitent les anomalies dans les graphes, et ce vu leur expressivité et leur capacité à représenter la complexité d'interaction du monde réel (Akoglu et al., 2015). Les techniques présentées dans la suite de cette section, discernent les anomalies à partir de la structure topologique d 'un graphe monodimensionnel.
La topologie d'un réseau est porteuse d 'informations implicites décisives pour repérer les anomalies. À titre d'exemple, dans les réseaux sociaux, plusieurs types d'anomalies peuvent avoir lieu (Savage et al., 2014). Particulièrement, les spammeurs sont incapables de cacher un certain type de métadonnées telles que leurs patrons d'interactions, c'est-à-dire, les liens qu 'ils établissent et qui sont, entre autres, révélateurs de leur comportement irrégulier. Pour identifier ces acteurs malveillants du réseau, l'utilisation unique des données est insuffisante et il est nécessaire de considérer la structure topologique du réseau. Dans (Fathaliani et Bouguessa, 2015), un score d'anomalie qui évalue la proportion des liens émis et des liens reçus a été proposé pour identifier ce type d'utilisateurs malintentionnés.
À cela s'ajoute, dans les graphes, la détection de communautés qui a été appliquée pour la détection des intrusions dans les réseaux (Ding et al. , 2012). En effet, la détection de communautés (appelée aussi partitionnement) compte parmi les problématiques les plus étudiées dans les graphes. Explicitement, la détection de communautés consiste à découvrir la structure sous-jacente d'un réseau à savoir l'identification des nœuds fortement connectés entre eux (c'est-à-dire, les clusters). Dans ce contexte, les intrusions sont les nœuds présents dans une communauté, mais qui n'y appartiennent pas réellement. D'autres techniques de détection de communautés se sont concentrées sur l'identification des nœuds superflus ( outliers) qui sont marginalement connectés aux communautés. Ces nœuds forment un bruit dans le réseau et leur élimination peut, potentiellement , améliorer les résultats d'analyse . Un nombre d'algorithmes a été proposé à ce suj et, à savoir SCAN (Xu et al., 2007), gSkeletonClu (Huang et al., 2013) et SHRINK (Huang et al., 2010). L'algorithme SCAN détecte les communautés et les nœuds qui y sont marginalement connectés en se basant sur une métrique de similarité structurale qui se calcule entre les nœuds du graphe. Cette métrique tient compte de la connectivité entre les nœuds, et ce en examinant leurs voisinages. Plus les nœuds partagent des voisins, plus la valeur de la métrique de similarité est élevée. Pour déterminer les communautés et les outliers, un seuil minimal de similarité doit être fixé. De cette manière, les nœuds qui ont plusieurs voisins en commun se groupent dans une même communauté. Par ailleurs, les nœuds superflus sont les nœuds qui ne se sont pas affectés à des communautés et qui ont des valeurs de similarité faibles.
L'algorithme SCAN nécessite un paramétrage pour pouvoir identifier les communautés et les nœuds superflus. Pour éviter cette intervention supervisée, l'algorithme gSkeletonClu a été proposé. Il convient de préciser que, tout comme SCAN, l'algorithme gSkeletonClu est un algorithme de partitionnement à base de densité qui détermine aussi bien les communautés tous les outliers selon une métrique de similarité. Toutefois, l'algorithme gSkeletonClu permet une sélection automatique de la valeur du seuil minimal, et ce en maximisant une mesure de validité comme la modularité. La modularité est une métrique fréquemment utilisée pour mesurer la qualité de partitionnement d'un graphe. Quelques algorithmes comme gSkeletonClu utilisent cette métrique comme une fonction objective pour optimiser le partitionnement d'un graphe. Ici, il est utile de noter que , dans gSkeletonClu, l'extraction des communautés et des outliers peut se faire aussi avec un seuil spécifié à l'avance par l'utilisateur.
Dans la littérature, nous trouvons, également, SHRINK qui est un algorithme de partitionnement hiérarchique. Cet algorithme identifie les communautés et les
outliers sans la nécessité de paramétrage en reposant sur le principe de réduction. Plus précisément, l'algorithme commence par identifier les pairs de nœuds denses, c.-à-d. les nœuds dont la similarité structurale est maximale par rapport à leurs voisinages. Ensuite, une fusion se fait entre les pairs de nœuds denses identifiés pour construire itérativement des microcommunautés. De ce fait, une microcommunauté peut être un nœud isolé ou un sous-graphe d 'un ou plusieurs pairs de nœuds denses connectés. Au fur et à mesure des itérations, un arbre hiérarchique d'emboîtement des microcommunautés se construit.

## 4. Isolation-based

le principe de l'isolation-based approche consiste à isoler les observations anormales de l'ensemble de données. Données d'anomalies
sont censés être très différents des normaux. Ils sont également censés représentent une très petite proportion de l'ensemble des données. Ainsi, ils sont susceptibles être rapidement isolé. Certaines méthodes sont basées sur l'isolement. Les méthodes basées sur l'isolement sont différentes des autres statistiques, clustering ou plus proches approches voisines car elles ne calculent pas une distance ou une densité à partir de l'ensemble de données. Par conséquent, ils ont une complexité moindre et sont plus évolutifs. Ils ne souffrent pas du problème de CPU, de mémoire ou de consommation de temps. Ainsi, 
les méthodes basées sur la lation sont adaptées au contexte du flux de données.

## 5. Deep learning 

il représente une classe d’algorithmes d’apprentissage automatique supervisé ou non supervisé basés sur l’utilisation de plusieurs couches d’unité de traitement non linéaire. Parmi ces méthodes on cite les auto-encoders (AE) et One-Class Neural Networks (OC-NN) (Chalapathy et Chawla (2019)).

## 6. Autres techniques

Basées sur les machines à vecteurs de support (Schölkopf et al. (2000), les réseaux de neurones (Hodge et Austin
(2004)), les méthodes adaptées aux grandes dimensions par construction de sous-espaces ou par réduction de dimension (Aggarwal (2017)).


## 7. Récapitulatif de la Classification des différentes techniques de détection d’anomalies

 <!-- ![Classification des différentes techniques de détection d’anomalies](/images/recapt-detect-anom.png) -->

<p align="center">       
        <img src="images/recapt-detect-anom.png" alt="Classification des différentes techniques de détection d’anomalies"> 
</p>

## 8.Classification of data stream anomaly detection methods

<p align="center">       
        <img src="images/recap-data-stream-anom.png" alt="Classification des différentes techniques de détection d’anomalies"> 
</p>



## Méthodes  expérimentales de détection d’anomalies très utilisées dans la littérature

Méthodes se basant à la fois sur le type de jeux de données (flux, séries temporelles, graphes, etc.), le domaine d’application et l’approche considérée (statistique, classification, clustering, etc.). Trois algorithmes : 
- **LOF**, Local Outlier Factor (LOF) est une méthode phare de la détection d’anomalies locales basée sur la densité de l’observation en question par rapport à la densité de ses plus proches voisins. Proposée par Breunig et al. (2000), LOF est une méthode non supervisée qui donne un score représentant le degré d’aberrance de l’observation. Les observations dont le degré d’aberrance est largement supérieur à 1 sont considérées comme anomalies. 
- **OC-SVM**, One-Class Support Vector Machine (OC-SVM) est une méthode de détection d’anomalies qui applique des algorithmes de SVM au problème de One class classification (OCC) proposée par Schölkopf et al. (2000, 2001). Le séparateur à vaste marge (SVM) appelé aussi machine à vecteurs de support est très utilisé pour l’apprentissage automatique du fait de sa puissance et de sa polyvalence (classification linéaire, non-linéaire, régression). OCC est une approche de classification semi-supervisée qui consiste à repérer toutes les observations appartenant à une classe précise connue pendant l’apprentissage, dans tout le jeu de données. L’idée clé de cette méthode est de trouver un hyperplan dans un espace de grande dimension qui sépare les anomalies des données normales.
- **Isolation Forest**, IForest est une méthode basée sur les arbres de décision et les forêts aléatoires(Liu et al. (2008, 2012)). Elle utilise l’isolation d’observations à partir de la construction de plusieurs arbres aléatoires. Quand une forêt d’arbres aléatoires et indépendants produit collectivement un chemin d’accès court pour atteindre une observation depuis la racine, celle-ci a une forte probabilité d’être une anomalie. Le nombre d’arbres utilisés est donc un important paramètre pour IForest. Le seuil de la détection est aussi un paramètre clé, il est donné par le score calculé pour chaque observation relativement aux autres observations.

Nous privilégierons une approche basée sur l'isolement : Isolation Forest.

À ce propos vérifions si ces programmes en langage C++ ne traitent pas déjà la question

  1. [ISOTREE](https://github.com/antaresatlantide/anomalie-detection/blob/main/isotree.md)
  2. [LIBISOLATIONFOREST](https://github.com/antaresatlantide/anomalie-detection/blob/main/LibIsolationForest.md)
  3. [RANGER](https://github.com/antaresatlantide/anomalie-detection/blob/main/ranger.md)
  4. [Machine Learning From Scratch with C++](https://github.com/antaresatlantide/anomalie-detection/blob/main/MLfromcrashcpp.md)
     - 4.1 [K-Means](https://github.com/antaresatlantide/anomalie-detection/blob/main/MLfromcrashcpp.md)
     - 4.2 [K-NN](https://github.com/antaresatlantide/anomalie-detection/blob/main/MLfromcrashcpp.md)
     - 4.3 [REGRESSIONTREES](https://github.com/antaresatlantide/anomalie-detection/blob/main/MLfromcrashcpp.md)
    
  
