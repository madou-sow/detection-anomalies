# Détection d'Anomalies

**Proposition de Christophe C.**

Dans la continuation de notre travail avec Kimura, nous allons maintenant passer au data stream anomaly detection problem.

Il y a en gros 3 techniques pour a détection d’anomalies, qui sont parfaitement résumées dans le papier suivant qui attaque la question du « online » (comme nous pour le clustering) : https://hal.archives-ouvertes.fr/hal-02874869v2/file/IForestASD_ScikitMultiflow_Version2_PourHAL.pdf

Il faudrait faire une implémentation en C de ce qui est proposé dans cet article. Sans doute en réduisant la dimension des données en entrée qui pourraient être des données 2D comme on a fait pour le clustering.

Notre fond de commerce c’est de pouvoir exécuter sur des devices comme celui de Terasic mentionné ci-après. Tu vas reconnaître des SDK pour ce type de board.

L’article sur HAL, lui, suppose que nous travaillons dans l’écosystème Python (voir la communauté Tinyml (https://www.tinyml.org/) , voir micropython (https://micropython.org/), voir river cité dans l’article https://github.com/online-ml/river/ et https://scikit-multiflow.github.io/). C’est la vision un peu orthogonale à la notre. Nous n’allons pas aller dans cette direction.

# Détails de l'Article Mariam Barry & al.

https://github.com/MariamBARRY

1. iForestASD_Streaming_Scikit-MultiFlow

2. skmultiflow_IForestASD

3. streaming-online-learning

## iForestASD_Streaming_Scikit-MultiFlow
[**iForestASD_Streaming_Scikit-MultiFlow.py**](python/iforestasd_scikitmultiflow.py)

<!--
        # -*- coding: utf-8 -*-
      """Notebook_test_iForestASD_Scikitmultiflow.ipynb

      http://localhost:8888/edit/cerin24022022/py-mariambarry/iForestASD_Streaming_Scikit-MultiFlow/Notebook_24032020_2325/iforestasd_scikitmultiflow.py

      Automatically generated by Colaboratory.

      Original file is located at
          https://colab.research.google.com/drive/1APZBgZ0fuHufYWM5QKqFhR7cpPs0bF2T

      # iForestASD :  Unsupervised Anomaly Detection with Scikit-MultiFlow

      An Implementation of Unsupervised Anomaly Detection with Isolation Forest in Scikit-MultiFlow with Sliding Windows \& drift detection


      ## References :

       - An Anomaly Detection Approach Based on Isolation Forest  for Streaming Data using Sliding Window (Ding \& Fei, 2013) https://www.sciencedirect.com/science/article/pii/S1474667016314999

       - Isolation-based Anomaly Detection (Liu, Ting \& Zhou, 2011) https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/tkdd11.pdf

       - Scikit MultiFlow HalfSpace Trees Implementation - “Fast anomaly detection for streaming data,” in IJCAI Proceedings - S.C.Tan, K.M.Ting, and T.F.Liu, 
       https://scikit-multiflow.github.io/scikit-multiflow/_autosummary/skmultiflow.anomaly_detection.HalfSpaceTrees.html#id2

       - Original implementation of Isolation Forest (not the one in SK-learn) https://github.com/Divya-Bhargavi/isolation-forest
      """



      """# Notebook File Structure  is the following

      Part 1 - Main Class contains
        - Init,
        - Partial_fit,
        - Update_model,
        - Predict methods which use the anomaly_score methods of the iForest class

      Part 2 - Isolation Forest class (re-used) and main functions
       - 

      Part 3 - Testing some examples and comparison of HS-Trees and IsolatationForestStream 
      - on synthetic 
      - on Real (HTTP) data.

      ## Import lib and packages
      """



      """## Install Cyphion then load the Scikit-multiflow latest release from Github"""

      ## !pip install scikit-multiflow



      #!pip install -U git+https://github.com/scikit-multiflow/scikit-multiflow

      from skmultiflow.core import BaseSKMObject, ClassifierMixin

      from skmultiflow.utils import check_random_state

      import numpy as np

      import random


      """# Part 1 - Main class - IsolationForestStream"""

      ## To implement this class, we took inspiration from Scikit-MultiFLow HSTrees implementation to follow its requirements.

      class IsolationForestStream(BaseSKMObject, ClassifierMixin):

        """
        This code implements  Anomaly Detection Approach Based on Isolation Forest Algorithm for Streaming Data Using Sliding Window (Ding \& Fei, 2013) [3]

          Each sample has an anomaly score is computed based on Isolation Forest anomaly based approach [2]. The concept of Isolation forest [1]
          consists on  isolating observations by randomly selecting a feature
          and then randomly selecting a split value between the maximum and minimum
          values of the selected feature.

          Model is updated of a Drift has been detected based on a input drift threshold. The drift detection approach is proposed by [2] 
          and works as follow : if the averaged anomaly score between two successive sliding windows is highter than the drift threshold (u), 
          then the previous model is completely discarded and a new model is build as an isolation forest on latest sliding windows stream.


        Parameters

          ---------

          n_estimators: int, optional (default=25)

             Number of trees in the ensemble.

             't' in the original paper.



          window_size: int, optional (default=100)

              The window size of the stream.

              ψ, 'Psi' in the original paper.   

      ## Optional       

          anomaly_threshold: double, optional (default=0.5)

              The threshold for declaring anomalies.

              Any instance prediction probability above this threshold will be declared as an anomaly.

          drift_threshold: double, optional (default=0.5)

              The threshold for detecting Drift and update the model.

             If the averaged anomaly score between two successive sliding windows is highter than the threshold (u), 
          then the previous model is completely discarded and a new model is build as an isolation forest on latest sliding windows stream.
          This parameters is supposed to be know by an expert domain, depending on data set.

      ## Other Attributes

          ensemble : Isolation Tree Ensemble

              Contain an Isolation Tree Ensemble object, current model for   IsolationForestStream

          sample_size : int

              Number of sample seen since the update

          anomaly_rate : float

              Rate of the anomalies in the previous sliding window (AnomalyRate in the original paper iForestASD)

          prec_window & window : numpy.ndarray of shape (n_samples, self.window_size)

              The previous and current window of data

          cpt : int

              Counter, if the n_estimator is higher than its, it will fit

          References
          ----------

          [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua.        
      “Isolation forest.” Data Mining, 2008. ICDM’08. Eighth IEEE International Conference on.

          [2] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation-based anomaly detection.” ACM Transactions on Knowledge Discovery from Data (TKDD) 6.1 (2012): 
      self.n_estimators

          [3] Ding, Zhiguo. (2013) An Anomaly Detection Approach Based on Isolation Forest Algorithm for Streaming Data Using Sliding Window. 12-17. 10.3182/20130902-3-CN-3020.00044. 

          """ 

        def __init__(self, window_size=100, n_estimators=25, anomaly_threshold=0.5, drift_threshold=0.5, random_state=None):

              super().__init__()

              self.n_estimators = n_estimators

              self.ensemble = None

              self.random_state = random_state

              self.window_size = window_size

              self.samples_seen = 0

              self.anomaly_rate = 0.20 

              self.anomaly_threshold = anomaly_threshold

              self.drift_threshold = drift_threshold

              self.window = None

              self.prec_window = None

              self.cpt = 0


        def partial_fit(self, X, y, classes=None, sample_weight=None):

                """ Partially (incrementally) fit the model.
                Parameters
                ----------
                X : numpy.ndarray of shape (n_samples, n_features)
                    The features to train the model.
                y: numpy.ndarray of shape (n_samples)
                    An array-like with the class labels of all samples in X.
                classes: None
                    Not used by this method.
                sample_weight: None
                    Not used by this method.
                Returns
                -------
                self
                """

                ## get the number of observations
                number_instances, _ = X.shape

                if(self.samples_seen==0):
                  ## ToDo ? Give a sample of self.window_size in attribute of iForest
                  iforest = IsolationTreeEnsemble(self.window_size,self.n_estimators,self.random_state)
                  self.ensemble = iforest


                for i in range(number_instances):
                    self._partial_fit(X[i], y[i])

                return self


        def _partial_fit(self, X, y):

                """ Trains the model on samples X and corresponding targets y.
                Private function where actual training is carried on.
                Parameters
                ----------
                X: numpy.ndarray of shape (1, n_features)
                    Instance attributes.
                y: int
                    Class label for sample X. Not used in this implementaion which is Unsupervised
                """ 

                """
                Reshape X and add it to our window if it isn't full.
                If it's full, give window to our precedent_window.
                If we are at the end our window, fit if we're learning 
                Check the anomaly score of our window 
                Update if self.anomaly_rate > self.drift_threshold

                """
                X = np.reshape(X,(1,len(X)))

                if self.samples_seen % self.window_size == 0:
                  ## Update the two windows (precedent one and current windows)
                  self.prec_window = self.window
                  self.window = X
                else:
                  self.window = np.concatenate((self.window,X))


                if self.samples_seen % self.window_size == 0 and self.samples_seen !=0:
                    #Fit the ensemble if it's not empty
                    if(self.cpt<self.n_estimators):
                      self.ensemble.fit(self.prec_window)
                      self.cpt += 1                  
                        ## Update the current anomaly score
                    self.anomaly_rate = self.anomaly_scores_rate(self.prec_window) ## Anomaly rate
                    #print(self.anomaly_rate) ## 

                        ## Update the model if the anomaly rate is greater than the threshold (u in the original paper [3])
                    if self.anomaly_rate > self.drift_threshold: ## Use Anomaly RATE ?
                      self.update_model(self.prec_window) # This function will discard completly the old model and create a new one

                self.samples_seen += 1

        def update_model(self,window):
          """ Update the model (fit a new isolation forest) if the current anomaly rate (in the previous sliding window)
           is higher than self.drift_threshold
              Parameters: 
                window: numpy.ndarray of shape (self.window_size, n_features)
              Re-Initialize our attributes and our ensemble, fit with the current window

          """

          ## ToDo ? Give a sample of self.window_size in attribute of iForest
          self.is_learning_phase_on = True
          iforest = IsolationTreeEnsemble(self.window_size,self.n_estimators,self.random_state)
          self.ensemble = iforest
          self.ensemble.fit(window)
          print("iForest ASD Update the model by training a new iForest")


        def anomaly_scores_rate(self, window):
          """
          Given a 2D matrix of observations, compute the anomaly rate 
          for all instances in the window and return an anomaly rate of the given window.

          Parameters :
          window: numpy.ndarray of shape (self.window_size, n_features)
          """

          score_tab = 2.0 ** (-1.0 * self.ensemble.path_length(window) / c(len(window)))
          score = 0
          for x in score_tab:
            if x > self.anomaly_threshold:
              score += 1
          return score / len(score_tab)

        def predict(self, X):
          """
          Given an instance, Predict the anomaly (1 or 0) based on the last sample of the window by using predict_proba if our model have fit, 
          else return None

          """
          if(self.samples_seen <= self.window_size):

            return [-1] ## Return the last element

          X = np.reshape(X,(1,len(X[0])))
          self.prec_window = np.concatenate((self.prec_window ,X)) ## Append the instances in the sliding window

          prediction =  self.ensemble.predict_from_anomaly_scores(self.predict_proba(self.prec_window),self.anomaly_threshold) ## return 0 or 1

          return [prediction]

        def predict_proba(self, X):
          """
          Calculate the anomaly score of the window if our model have fit, else return None
          Parameters :
          X: numpy.ndarray of shape (self.window_size, n_features)   

          """
          if(self.samples_seen <= self.window_size):
              return [-1]
          return self.ensemble.anomaly_score(self.prec_window)[-1] # Anomaly return an array with all scores of each data, taking -1 return the last instance (X) anomaly score

      """# Part 2- IsolationTreeEnsemble  Class (iForest in the original paper)"""

      # Follows original paper algo from https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf
      # Original Source re-used and adpted to our project from https://github.com/Divya-Bhargavi/isolation-forest 
      class IsolationTreeEnsemble:
          def __init__(self, sample_size, n_trees, random_state):
              self.sample_size = sample_size
              self.n_trees = n_trees
              self.depth = np.log2(sample_size)
              self.trees = []
              self.random_state = random_state
              self._random_state = check_random_state(self.random_state)
              self.is_learning_phase_on = True 

          def fit(self, X:np.ndarray):
              """
              Given a 2D matrix of observations, create an ensemble of IsolationTree
              objects and store them in a list: self.trees.  Convert DataFrames to
              ndarray objects.
              """
              len_x = len(X)

              for i in range(self.n_trees):
                  sample_idx = random.sample(list(range(len_x)), self.sample_size )
                  temp_tree = IsolationTree(self.depth, 0).fit(X[sample_idx])
                  self.trees.append(temp_tree)

              return self

          def path_length(self, X:np.ndarray):
              """
              Given a 2D matrix of observations, X, compute the average path length
              for each observation in X.  Compute the path length for x_i using every
              tree in self.trees then compute the average for each x_i.  Return an
              ndarray of shape (len(X),1).
              """
              pl_vector = []

              for x in (X):
                  pl = np.array([path_length_tree(x, t, 0) for t in self.trees])
                  pl = pl.mean()

                  pl_vector.append(pl)

              pl_vector = np.array(pl_vector).reshape(-1, 1)

              return pl_vector

          def anomaly_score(self, X:np.ndarray):
              """
              Given a 2D matrix of observations, X, compute the anomaly score
              for each x_i observation, returning an ndarray of them.
              """
              return 2.0 ** (-1.0 * self.path_length(X) / c(len(X)))

          def predict_from_anomaly_scores(self, scores:int, threshold:float):
              """
              Given an array of scores and a score threshold, return an array of
              the predictions: 1 for any score >= the threshold and 0 otherwise.
              """
              predictions = 1 if scores >= threshold else 0

              return predictions


      class IsolationTree:
          def __init__(self, height_limit, current_height):

              self.depth = height_limit
              self.current_height = current_height
              self.split_by = None
              self.split_value = None
              self.right = None
              self.left = None
              self.size = 0
              self.exnodes = 0
              self.n_nodes = 1

          def fit(self, X:np.ndarray):
              """
              Given a 2D matrix of observations, create an isolation tree. Set field
              self.root to the root of that tree and return it.
              If you are working on an improved algorithm, check parameter "improved"
              and switch to your new functionality else fall back on your original code.
              """

              if len(X) <= 1 or self.current_height >= self.depth:
                  self.exnodes = 1
                  self.size = X.shape[0]

                  return self

              split_by = random.choice(np.arange(X.shape[1]))
              X_col = X[:, split_by]
              min_x = X_col.min()
              max_x = X_col.max()

              if min_x == max_x:
                  self.exnodes = 1
                  self.size = len(X)

                  return self

              else:

                  split_value = min_x + random.betavariate(0.5, 0.5) * (max_x - min_x)

                  w = np.where(X_col < split_value, True, False)
                  del X_col

                  self.size = X.shape[0]
                  self.split_by = split_by
                  self.split_value = split_value

                  self.left = IsolationTree(self.depth, self.current_height + 1).fit(X[w])
                  self.right = IsolationTree(self.depth, self.current_height + 1).fit(X[~w])
                  self.n_nodes = self.left.n_nodes + self.right.n_nodes + 1

              return self

      def c(n):
          if n > 2:
              return 2.0*(np.log(n-1)+0.5772156649) - (2.0*(n-1.)/(n*1.0))
          elif n == 2:
              return 1
          if n == 1:
              return 0

      def path_length_tree(x, t,e):
          e = e
          if t.exnodes == 1:
              e = e+ c(t.size)
              return e
          else:
              a = t.split_by
              if x[a] < t.split_value :
                  return path_length_tree(x, t.left, e+1)
              if x[a] >= t.split_value :
                  return path_length_tree(x, t.right, e+1)
 
-->

[**functions.py**](python/functions.py)

<!--
                #!/usr/bin/env python3
                # -*- coding: utf-8 -*-
                """
                Created on Sun Mar 22 20:22:15 2020

                @author: http://localhost:8888/edit/cerin24022022/py-mariambarry/iForestASD_Streaming_Scikit-MultiFlow/Notebook_24032020_2325/functions.py
                """

                class Comparison:

                  def __init__(self):

                        super().__init__()

                  #The goal of this function is to execute the models and show the differents results. 
                  #It is the function to call when we want to test differents models 
                  #with differents values for parameters
                  def run_comparison(self, stream, stream_n_features, window = 100, 
                                     estimators = 50, anomaly = 0.5, drift_rate = 0.3, 
                                     result_folder="Generated", max_sample=100000, n_wait=200,
                                     metrics=['accuracy', 'f1', 'kappa', 'kappa_m', 
                                              'running_time','model_size']):

                    from skmultiflow.anomaly_detection import HalfSpaceTrees
                    from iforestasd_scikitmultiflow import IsolationForestStream
                    from skmultiflow.evaluation.evaluate_prequential import EvaluatePrequential

                    # Creation f the result csv
                    # sow directory_path = 'results/'+str(result_folder)
                    directory_path = '/home/mamadou/big-data/cerin24022022/mariambarry/iForestASD_Streaming_Scikit-MultiFlow/Notebook_24032020_2325/results/'+str(result_folder)
                    self.check_directory(path=directory_path)
                    result_file_path = directory_path+'/result_for_WS'+str(window)+'_NE'+str(estimators)+'.csv'

                    # 2. Prepare for use This function is usefull to have data window by window
                    # stream.prepare_for_use() # Deprecated so how to prepare data?

                    models = [HalfSpaceTrees(n_features=stream_n_features, window_size=window, 
                                             n_estimators=estimators, anomaly_threshold=anomaly),
                    #IForest ASD use all the window_size for the sample in the training phase
                    IsolationForestStream(window_size=window, n_estimators=estimators, 
                                          anomaly_threshold=anomaly, drift_threshold=drift_rate)]
                    # Setup the evaluator
                    evaluator = EvaluatePrequential(pretrain_size=1, max_samples=max_sample, 
                                                    show_plot=True, 
                                                    metrics=metrics, batch_size=1, 
                                                    output_file = result_file_path,
                                                    n_wait = n_wait) 
                    # 4. Run the evaluation 
                    evaluator.evaluate(stream=stream, model=models, model_names=['HSTrees','iForestASD'])
                    print("")
                    print("Please find evaluation results here "+result_file_path)
                    return 

                  def get_dataset(self, dataset_name="Generator", classification_function=0, 
                                  noise_percentage=0.7, random_state=1):
                      #Dataset
                      #  Name M(#instances) N(#attributes) Anomaly
                      #  Threshold
                      #  Http 567498 3 0.39%
                      #  Smtp 95156 3 0.03%
                      #  ForestCover 286048 10 0.96%
                      #  Shuttle 49097 9 7.15%
                      if dataset_name=="Generator":
                         return self.get_data_generated(classification_function, 
                                                        noise_percentage, random_state);
                      elif dataset_name=="HTTP":
                         path = "/home/mamadou/big-data/cerin24022022/mariambarry/iForestASD_Streaming_Scikit-MultiFlow/Notebook_24032020_2325/datasets/HTTP.csv"
                         return self.get_file_stream(path);
                      elif dataset_name=="ForestCover":
                         path = "/home/mamadou/big-data/cerin24022022/mariambarry/iForestASD_Streaming_Scikit-MultiFlow/Notebook_24032020_2325/datasets/ForestCover.csv"
                         return self.get_file_stream(path);
                      elif dataset_name=="Shuttle":
                         path = "/home/mamadou/big-data/cerin24022022/mariambarry/iForestASD_Streaming_Scikit-MultiFlow/Notebook_24032020_2325/datasets/Shuttle.csv"
                         return self.get_file_stream(path);
                      elif dataset_name=="SMTP":
                         path = "/home/mamadou/big-data/cerin24022022/mariambarry/iForestASD_Streaming_Scikit-MultiFlow/Notebook_24032020_2325/datasets/SMTP.csv"
                         return self.get_file_stream(path);
                      else:
                         print("The specified dataset do not exist yet."+ 
                               " Try to contact the administrator for any add. "+
                               " Or choose between these datasets:['Generator','HTTP','ForestCover','Shuttle','SMTP']");
                         return None

                  def get_file_stream(self, path):
                      from skmultiflow.data.file_stream import FileStream
                      return FileStream(path, n_targets=1, target_idx=-1)

                  def get_data_stream(self, path):
                      from skmultiflow.data.data_stream import DataStream
                      return


                  def get_data_generated(self,classification_function, noise_percentage, random_state):
                      from skmultiflow.data import SEAGenerator
                      return SEAGenerator(classification_function=classification_function, 
                                          noise_percentage=noise_percentage, random_state=random_state)


                  #To transform datasets by replace anomaly label by 1 and normal label by 0
                  def prepare_dataset_for_anomaly(self, full_dataset, y_column:int, 
                                                  anomaly_label:str='\'Anomaly\'', file_name:str="new"):
                      import numpy as np
                      import pandas as pd
                      full_dataset[y_column] = np.where(full_dataset[y_column]==anomaly_label,1,0)
                      dataset = pd.DataFrame(full_dataset)
                      dataset.drop([0], inplace=True)
                      full_file_path = "/home/mamadou/big-data/cerin24022022/mariambarry/iForestASD_Streaming_Scikit-MultiFlow/Notebook_24032020_2325/datasets/"+file_name+".csv"
                      dataset.to_csv(full_file_path, index=None, header=True)
                      return dataset

                  def check_directory(self,path):
                      from pathlib import Path
                      Path(path).mkdir(parents=True, exist_ok=True) 
-->
                      
## skmultiflow_IForestASD

[**iforestasd_scikitmultiflow.py**](python/iforestasd_scikitmultiflow1.py)

<!--
                        # -*- coding: utf-8 -*-
                        """Notebook_test_iForestASD_Scikitmultiflow.ipynb

                        mamadou@port-lipn12:~/big-data/cerin24022022/py-mariambarry/skmultiflow_IForestASD/source/iforestasd_scikitmultiflow.py

                        Automatically generated by Colaboratory.

                        Original file is located at
                        https://colab.research.google.com/drive/1APZBgZ0fuHufYWM5QKqFhR7cpPs0bF2T

                        # iForestASD :  Unsupervised Anomaly Detection with Scikit-MultiFlow

                        An Implementation of Unsupervised Anomaly Detection with Isolation Forest in Scikit-MultiFlow with Sliding Windows \& drift detection


                        ## References :

                        - An Anomaly Detection Approach Based on Isolation Forest  for Streaming Data using Sliding Window (Ding \& Fei, 2013) https://www.sciencedirect.com/science/article/pii/S1474667016314999

                        - Isolation-based Anomaly Detection (Liu, Ting \& Zhou, 2011) https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/tkdd11.pdf

                        - Scikit MultiFlow HalfSpace Trees Implementation - “Fast anomaly detection for streaming data,” in IJCAI Proceedings - S.C.Tan, K.M.Ting, and T.F.Liu, 
                        https://scikit-multiflow.github.io/scikit-multiflow/_autosummary/skmultiflow.anomaly_detection.HalfSpaceTrees.html#id2

                        - Original implementation of Isolation Forest (not the one in SK-learn) https://github.com/Divya-Bhargavi/isolation-forest
                        """



                        """# Notebook File Structure  is the following

                        Part 1 - Main Class contians
                        - Init,
                        - Partial_fit,
                        - Update_model,
                        - Predict methods which use the anomaly_score methods of the iForest class

                        Part 2 - Isolation Forest class (re-used) and main functions
                        - 

                        Part 3 - Testing some examples and comparison of HS-Trees and IsolatationForestStream 
                        - on synthetic 
                        - on Real (HTTP) data.

                        ## Import lib and packages
                        """



                        """## Install Cyphion then load the Scikit-multiflow latest release from Github"""

                        ## !pip install scikit-multiflow



                        #!pip install -U git+https://github.com/scikit-multiflow/scikit-multiflow

                        from skmultiflow.core import BaseSKMObject, ClassifierMixin

                        from skmultiflow.utils import check_random_state

                        import numpy as np

                        import random


                        """# Part 1 - Main class - IsolationForestStream"""

                        ## To implement this class, we took inspiration from Scikit-MultiFLow HSTrees implementation to follow its requirements.

                        class IsolationForestStream(BaseSKMObject, ClassifierMixin):

                        """
                        This code implements  Anomaly Detection Approach Based on Isolation Forest Algorithm for Streaming Data Using Sliding Window (Ding \& Fei, 2013) [3]

                        Each sample has an anomaly score is computed based on Isolation Forest anomaly based approach [2]. The concept of Isolation forest [1]
                        consists on  isolating observations by randomly selecting a feature
                        and then randomly selecting a split value between the maximum and minimum
                        values of the selected feature.

                        Model is updated of a Drift has been detected based on a input drift threshold. The drift detection approach is proposed by [2] 
                        and works as follow : if the averaged anomaly score between two successive sliding windows is highter than the drift threshold (u), 
                        then the previous model is completely discarded and a new model is build as an isolation forest on latest sliding windows stream.


                        Parameters

                        ---------

                        n_estimators: int, optional (default=25)

                        Number of trees in the ensemble.

                        't' in the original paper.



                        window_size: int, optional (default=100)

                        The window size of the stream.

                        ψ, 'Psi' in the original paper.   

                        ## Optional       

                        anomaly_threshold: double, optional (default=0.5)

                        The threshold for declaring anomalies.

                        Any instance prediction probability above this threshold will be declared as an anomaly.

                        drift_threshold: double, optional (default=0.5)

                        The threshold for detecting Drift and update the model.

                        If the averaged anomaly score between two successive sliding windows is highter than the threshold (u), 
                        then the previous model is completely discarded and a new model is build as an isolation forest on latest sliding windows stream.
                        This parameters is supposed to be know by an expert domain, depending on data set.

                        ## Other Attributes

                        ensemble : Isolation Tree Ensemble

                        Contain an Isolation Tree Ensemble object, current model for   IsolationForestStream

                        sample_size : int

                        Number of sample seen since the update

                        anomaly_rate : float

                        Rate of the anomalies in the previous sliding window (AnomalyRate in the original paper iForestASD)

                        prec_window & window : numpy.ndarray of shape (n_samples, self.window_size)

                        The previous and current window of data

                        cpt : int

                        Counter, if the n_estimator is higher than its, it will fit

                        References
                        ----------

                        [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua.        
                        “Isolation forest.” Data Mining, 2008. ICDM’08. Eighth IEEE International Conference on.

                        [2] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation-based anomaly detection.” ACM Transactions on Knowledge Discovery from Data (TKDD) 6.1 (2012): 
                        self.n_estimators

                        [3] Ding, Zhiguo. (2013) An Anomaly Detection Approach Based on Isolation Forest Algorithm for Streaming Data Using Sliding Window. 12-17. 10.3182/20130902-3-CN-3020.00044. 

                        """ 

                        def __init__(self, window_size=100, n_estimators=25, anomaly_threshold=0.5, drift_threshold=0.5, random_state=None):

                        super().__init__()

                        self.n_estimators = n_estimators

                        self.ensemble = None

                        self.random_state = random_state

                        self.window_size = window_size

                        self.samples_seen = 0

                        self.anomaly_rate = 0.20 

                        self.anomaly_threshold = anomaly_threshold

                        self.drift_threshold = drift_threshold

                        self.window = None

                        self.prec_window = None

                        self.cpt = 0


                        def partial_fit(self, X, y, classes=None, sample_weight=None):

                          """ Partially (incrementally) fit the model.
                          Parameters
                          ----------
                          X : numpy.ndarray of shape (n_samples, n_features)
                              The features to train the model.
                          y: numpy.ndarray of shape (n_samples)
                              An array-like with the class labels of all samples in X.
                          classes: None
                              Not used by this method.
                          sample_weight: None
                              Not used by this method.
                          Returns
                          -------
                          self
                          """

                          ## get the number of observations
                          number_instances, _ = X.shape

                          if(self.samples_seen==0):
                            ## ToDo ? Give a sample of self.window_size in attribute of iForest
                            iforest = IsolationTreeEnsemble(self.window_size,self.n_estimators,self.random_state)
                            self.ensemble = iforest


                          for i in range(number_instances):
                              self._partial_fit(X[i], y[i])

                          return self


                        def _partial_fit(self, X, y):

                          """ Trains the model on samples X and corresponding targets y.
                          Private function where actual training is carried on.
                          Parameters
                          ----------
                          X: numpy.ndarray of shape (1, n_features)
                              Instance attributes.
                          y: int
                              Class label for sample X. Not used in this implementaion which is Unsupervised
                          """ 

                          """
                          Reshape X and add it to our window if it isn't full.
                          If it's full, give window to our precedent_window.
                          If we are at the end our window, fit if we're learning 
                          Check the anomaly score of our window 
                          Update if self.anomaly_rate > self.drift_threshold

                          """
                          X = np.reshape(X,(1,len(X)))

                          if self.samples_seen % self.window_size == 0:
                            ## Update the two windows (precedent one and current windows)
                            self.prec_window = self.window
                            self.window = X
                          else:
                            self.window = np.concatenate((self.window,X))


                          if self.samples_seen % self.window_size == 0 and self.samples_seen !=0:
                              #Fit the ensemble if it's not empty
                              if(self.cpt<self.n_estimators):
                                self.ensemble.fit(self.prec_window)
                                self.cpt += 1                  
                                  ## Update the current anomaly score
                              self.anomaly_rate = self.anomaly_scores_rate(self.prec_window) ## Anomaly rate
                              #print(self.anomaly_rate) ## 

                                  ## Update the model if the anomaly rate is greater than the threshold (u in the original paper [3])
                              if self.anomaly_rate > self.drift_threshold: ## Use Anomaly RATE ?
                                self.update_model(self.prec_window) # This function will discard completly the old model and create a new one

                          self.samples_seen += 1

                        def update_model(self,window):
                        """ Update the model (fit a new isolation forest) if the current anomaly rate (in the previous sliding window)
                        is higher than self.drift_threshold
                        Parameters: 
                          window: numpy.ndarray of shape (self.window_size, n_features)
                        Re-Initialize our attributes and our ensemble, fit with the current window

                        """

                        ## ToDo ? Give a sample of self.window_size in attribute of iForest
                        self.is_learning_phase_on = True
                        iforest = IsolationTreeEnsemble(self.window_size,self.n_estimators,self.random_state)
                        self.ensemble = iforest
                        self.ensemble.fit(window)
                        #print("iForest ASD Update the model by training a new iForest")


                        def anomaly_scores_rate(self, window):
                        """
                        Given a 2D matrix of observations, compute the anomaly rate 
                        for all instances in the window and return an anomaly rate of the given window.

                        Parameters :
                        window: numpy.ndarray of shape (self.window_size, n_features)
                        """

                        score_tab = 2.0 ** (-1.0 * self.ensemble.path_length(window) / c(len(window)))
                        score = 0
                        for x in score_tab:
                        if x > self.anomaly_threshold:
                        score += 1
                        return score / len(score_tab)

                        def predict(self, X):
                        """
                        Given an instance, Predict the anomaly (1 or 0) based on the last sample of the window by using predict_proba if our model have fit, 
                        else return None

                        """
                        if(self.samples_seen <= self.window_size):

                        return [-1] ## Return the last element

                        X = np.reshape(X,(1,len(X[0])))
                        self.prec_window = np.concatenate((self.prec_window ,X)) ## Append the instances in the sliding window

                        prediction =  self.ensemble.predict_from_anomaly_scores(self.predict_proba(self.prec_window),self.anomaly_threshold) ## return 0 or 1

                        return [prediction]

                        def predict_proba(self, X):
                        """
                        Calculate the anomaly score of the window if our model have fit, else return None
                        Parameters :
                        X: numpy.ndarray of shape (self.window_size, n_features)   

                        """
                        if(self.samples_seen <= self.window_size):
                        return [-1]
                        return self.ensemble.anomaly_score(self.prec_window)[-1] # Anomaly return an array with all scores of each data, taking -1 return the last instance (X) anomaly score

                        """# Part 2- IsolationTreeEnsemble  Class (iForest in the original paper)"""

                        # Follows original paper algo from https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf
                        # Original Source re-used and adpted to our project from https://github.com/Divya-Bhargavi/isolation-forest 
                        class IsolationTreeEnsemble:
                        def __init__(self, sample_size, n_trees, random_state):
                        self.sample_size = sample_size
                        self.n_trees = n_trees
                        self.depth = np.log2(sample_size)
                        self.trees = []
                        self.random_state = random_state
                        self._random_state = check_random_state(self.random_state)
                        self.is_learning_phase_on = True 

                        def fit(self, X:np.ndarray):
                        """
                        Given a 2D matrix of observations, create an ensemble of IsolationTree
                        objects and store them in a list: self.trees.  Convert DataFrames to
                        ndarray objects.
                        """
                        len_x = len(X)

                        for i in range(self.n_trees):
                            sample_idx = random.sample(list(range(len_x)), self.sample_size )
                            temp_tree = IsolationTree(self.depth, 0).fit(X[sample_idx])
                            self.trees.append(temp_tree)

                        return self

                        def path_length(self, X:np.ndarray):
                        """
                        Given a 2D matrix of observations, X, compute the average path length
                        for each observation in X.  Compute the path length for x_i using every
                        tree in self.trees then compute the average for each x_i.  Return an
                        ndarray of shape (len(X),1).
                        """
                        pl_vector = []

                        for x in (X):
                            pl = np.array([path_length_tree(x, t, 0) for t in self.trees])
                            pl = pl.mean()

                            pl_vector.append(pl)

                        pl_vector = np.array(pl_vector).reshape(-1, 1)

                        return pl_vector

                        def anomaly_score(self, X:np.ndarray):
                        """
                        Given a 2D matrix of observations, X, compute the anomaly score
                        for each x_i observation, returning an ndarray of them.
                        """
                        return 2.0 ** (-1.0 * self.path_length(X) / c(len(X)))

                        def predict_from_anomaly_scores(self, scores:int, threshold:float):
                        """
                        Given an array of scores and a score threshold, return an array of
                        the predictions: 1 for any score >= the threshold and 0 otherwise.
                        """
                        predictions = 1 if scores >= threshold else 0

                        return predictions


                        class IsolationTree:
                        def __init__(self, height_limit, current_height):

                        self.depth = height_limit
                        self.current_height = current_height
                        self.split_by = None
                        self.split_value = None
                        self.right = None
                        self.left = None
                        self.size = 0
                        self.exnodes = 0
                        self.n_nodes = 1

                        def fit(self, X:np.ndarray):
                        """
                        Given a 2D matrix of observations, create an isolation tree. Set field
                        self.root to the root of that tree and return it.
                        If you are working on an improved algorithm, check parameter "improved"
                        and switch to your new functionality else fall back on your original code.
                        """

                        if len(X) <= 1 or self.current_height >= self.depth:
                            self.exnodes = 1
                            self.size = X.shape[0]

                            return self

                        split_by = random.choice(np.arange(X.shape[1]))
                        X_col = X[:, split_by]
                        min_x = X_col.min()
                        max_x = X_col.max()

                        if min_x == max_x:
                            self.exnodes = 1
                            self.size = len(X)

                            return self

                        else:

                            split_value = min_x + random.betavariate(0.5, 0.5) * (max_x - min_x)

                            w = np.where(X_col < split_value, True, False)
                            del X_col

                            self.size = X.shape[0]
                            self.split_by = split_by
                            self.split_value = split_value

                            self.left = IsolationTree(self.depth, self.current_height + 1).fit(X[w])
                            self.right = IsolationTree(self.depth, self.current_height + 1).fit(X[~w])
                            self.n_nodes = self.left.n_nodes + self.right.n_nodes + 1

                        return self

                        def c(n):
                        if n > 2:
                        return 2.0*(np.log(n-1)+0.5772156649) - (2.0*(n-1.)/(n*1.0))
                        elif n == 2:
                        return 1
                        if n == 1:
                        return 0

                        def path_length_tree(x, t,e):
                        e = e
                        if t.exnodes == 1:
                        e = e+ c(t.size)
                        return e
                        else:
                        a = t.split_by
                        if x[a] < t.split_value :
                            return path_length_tree(x, t.left, e+1)
                        if x[a] >= t.split_value :
                            return path_length_tree(x, t.right, e+1)
 -->                           
    
[**functions.py**](python/functions1.py)

<!--
                #!/usr/bin/env python3
                # -*- coding: utf-8 -*-
                """
                Created on Sun Mar 22 20:22:15 2020

                @author: mamadou@port-lipn12:~/big-data/cerin24022022/py-mariambarry/skmultiflow_IForestASD/source/

                functions.py

                An example of how to test our IForestASD Implementation & Compare IForestASD against HSTrees

                """

                class Comparison:

                  def __init__(self):

                        super().__init__()

                  #The goal of this function is to execute the models and show the differents results. 
                  #It is the function to call when we want to test differents models 
                  #with differents values for parameters
                  def run_comparison(self, stream, stream_n_features, window = 100, 
                                     estimators = 50, anomaly = 0.5, drift_rate = 0.3, 
                                     result_folder="Generated", max_sample=100000, n_wait=200,
                                     metrics=['accuracy', 'f1', 'kappa', 'kappa_m', 
                                              'running_time','model_size']):

                    from skmultiflow.anomaly_detection import HalfSpaceTrees
                    from source.iforestasd_scikitmultiflow import IsolationForestStream
                    from skmultiflow.evaluation.evaluate_prequential import EvaluatePrequential

                    # Creation f the result csv
                    directory_path = 'results/'+str(result_folder)
                    self.check_directory(path=directory_path)
                    result_file_path = directory_path+'/result_for_WS'+str(window)+'_NE'+str(estimators)+'.csv'

                    # 2. Prepare for use This function is usefull to have data window by window
                    # stream.prepare_for_use() # Deprecated so how to prepare data?

                    models = [HalfSpaceTrees(n_features=stream_n_features, window_size=window, 
                                             n_estimators=estimators, anomaly_threshold=anomaly),
                    #IForest ASD use all the window_size for the sample in the training phase
                    IsolationForestStream(window_size=window, n_estimators=estimators, 
                                          anomaly_threshold=anomaly, drift_threshold=drift_rate)]
                    # Setup the evaluator
                    evaluator = EvaluatePrequential(pretrain_size=1, max_samples=max_sample, 
                                                    show_plot=True, 
                                                    metrics=metrics, batch_size=1, 
                                                    output_file = result_file_path,
                                                    n_wait = n_wait) 
                    # 4. Run the evaluation 
                    evaluator.evaluate(stream=stream, model=models, model_names=['HSTrees','iForestASD'])
                    print("")
                    print("Please find evaluation results here "+result_file_path)
                    return 

                  def get_dataset(self, dataset_name="Generator", classification_function=0, 
                                  noise_percentage=0.7, random_state=1):
                      #Dataset
                      #  Name M(#instances) N(#attributes) Anomaly
                      #  Threshold
                      #  Http 567498 3 0.39%
                      #  Smtp 95156 3 0.03%
                      #  ForestCover 286048 10 0.96%
                      #  Shuttle 49097 9 7.15%
                      if dataset_name=="Generator":
                         return self.get_data_generated(classification_function, 
                                                        noise_percentage, random_state);
                      elif dataset_name=="HTTP":
                         path = "datasets/HTTP.csv"
                         return self.get_file_stream(path);
                      elif dataset_name=="ForestCover":
                         path = "datasets/ForestCover.csv"
                         return self.get_file_stream(path);
                      elif dataset_name=="Shuttle":
                         path = "datasets/Shuttle.csv"
                         return self.get_file_stream(path);
                      elif dataset_name=="SMTP":
                         path = "datasets/SMTP.csv"
                         return self.get_file_stream(path);
                      else:
                         print("The specified dataset do not exist yet."+ 
                               " Try to contact the administrator for any add. "+
                               " Or choose between these datasets:['Generator','HTTP','ForestCover','Shuttle','SMTP']");
                         return None

                  def get_file_stream(self, path):
                      from skmultiflow.data.file_stream import FileStream
                      return FileStream(path, n_targets=1, target_idx=-1)

                  def get_data_stream(self, path):
                      from skmultiflow.data.data_stream import DataStream
                      return


                  def get_data_generated(self,classification_function, noise_percentage, random_state):
                      from skmultiflow.data import SEAGenerator
                      return SEAGenerator(classification_function=classification_function, 
                                          noise_percentage=noise_percentage, random_state=random_state)


                  #To transform datasets by replace anomaly label by 1 and normal label by 0
                  def prepare_dataset_for_anomaly(self, full_dataset, y_column:int, 
                                                  anomaly_label:str='\'Anomaly\'', file_name:str="new"):
                      import numpy as np
                      import pandas as pd

                      full_dataset[y_column] = np.where(full_dataset[y_column]==anomaly_label,1,0)
                      dataset = pd.DataFrame(full_dataset)
                      dataset.drop([0], inplace=True)
                      full_file_path = "../datasets/"+file_name+".csv"
                      dataset.to_csv(full_file_path, index=None, header=True)
                      return dataset

                  def check_directory(self,path):
                      from pathlib import Path
                      Path(path).mkdir(parents=True, exist_ok=True) 


                  def merge_file(self, folder_path, output_file = 'output.csv'):
                    import os
                    import pandas as pd
                    result = pd.DataFrame()
                    print('List of file merged')
                    print()
                    no = '.ipynb_checkpoints'
                    for file_ in os.listdir(folder_path):
                        print(file_)
                        #list.append(file_)
                        if file_ != no:
                            print(file_)
                            df = pd.read_csv(folder_path+file_, sep = ',', skiprows=6, header = 0, dtype='unicode', error_bad_lines=False)
                            df.at[0,'param'] = str(file_)
                            df.at[0,'window'] = df.param.apply(lambda st: st[st.find("WS")+2:st.find("_NE")])[0]
                            df.at[0,'estimators']= df.param.apply(lambda st: st[st.find("NE")+2:st.find("_UP")])[0]
                            df.at[0,'updates']= df.param.apply(lambda st: st[st.find("UP_")+3:st.find(".csv")])[0]

                            result = pd.concat([result,df],ignore_index=True)
                    #result.sort_values(by = ['window', 'estimators'], inplace= True)
                    result.columns=df.columns
                    #output_file = 'RESULT_SHUTTLE10K.csv'
                    result.to_csv(output_file,index=False)

                    return result

                  def data_prep (self, df_forest):
                    df_forest.dropna(inplace= True)
                    df_forest.sort_values(by = ['window', 'estimators'], inplace= True)
                    df_forest.columns = df_forest.columns.str.replace('current_', '')
                    df_forest.drop(columns = ['param']).astype(float)
                    df_forest=df_forest.drop(columns = ['param']).astype(float)
                    df_forest.window = df_forest.window.astype(int)
                    df_forest.estimators = df_forest.estimators.astype(int)
                    df_forest['Windows_Trees_set_up']='W'+df_forest['window'].astype(str)+'__'+'T'+df_forest['estimators'].astype(str)
                    df_forest.columns = df_forest.columns.str.replace('current_', '')
                    df_forest.sort_values(by = ['window', 'estimators'], inplace= True)

                    return df_forest
--> 

## streaming-online-learning

Ce référentiel est l'implémentation officielle de l'article Online Learning Deployment for Streaming Applications in the Banking Sector (Barry, Montiel, Bifet, Chiky, Shakman, Manchev, Wadkar, El Baroudi, Tran, KDD 2021). Les ressources peuvent être utilisées pour configurer et déployer des instances de modèles d'apprentissage automatique en ligne, pour générer des prédictions et mettre à jour les poids du modèle sur les données en continu.

> **Motivations :** L'objectif est de proposer une plate-forme pour fournir un pont transparent entre les activités centrées sur la science des données et les activités d'ingénierie des données, de manière à satisfaire à la fois les contraintes de production imposées en termes d'évolutivité et les exigences des applications de streaming en termes d'apprentissage en ligne. Des exemples de cas d'utilisation potentiels peuvent être la détection d'anomalies et de fraudes pour les flux de données évoluant dans le temps ou la classification en temps réel des activités des utilisateurs ou des événements informatiques ou de journaux. Cela peut être un véritable accélérateur pour gagner en pro-activité dans la résolution de problèmes réels.

**Outils : RIVER, Kafka & Domino Platform on AWS**
> River est une bibliothèque d'apprentissage automatique en ligne open source écrite en Python dont l'objectif principal est l'apprentissage incrémentiel d'instance, ce qui signifie que chaque composant (estimateurs, transformateurs, métriques de performance, etc.) est conçu pour être mis à jour un échantillon à la fois. . Nous avons utilisé River pour former et mettre à jour en continu le modèle d'apprentissage en ligne à partir des derniers flux de données. KAFKA est une plate-forme de streaming d'événements distribués open source à la pointe de la technologie et nous avons utilisé un Kafka hébergé géré (confluent). Nous l'avons utilisé comme générateur de flux de données.

>La plate-forme Domino Platform est implémentée au-dessus de Kubernetes, où elle fait tourner des conteneurs à la demande pour exécuter les charges de travail des utilisateurs. Les conteneurs sont basés sur des images Docker, qui sont entièrement personnalisables. Nous avons utilisé Domino pour héberger les modèles et exécuter des tests d'évolutivité sur des données à haute vitesse générées sous forme de flux.

## Quelques définitions 

> **MicroPython :** 
>> MicroPython est une implémentation légère et efficace du langage de programmation Python 3 qui inclut un petit sous-ensemble de la bibliothèque standard Python et est optimisée pour s'exécuter sur des microcontrôleurs et dans des environnements contraints.

>> Le pyboard MicroPython est une carte électronique compacte qui exécute MicroPython sur le métal nu, vous offrant un système d'exploitation Python de bas niveau qui peut être utilisé pour contrôler toutes sortes de projets électroniques.

>> MicroPython regorge de fonctionnalités avancées telles qu'une invite interactive, des entiers de précision arbitraires, des fermetures, la compréhension de liste, des générateurs, la gestion des exceptions, etc. Pourtant, il est suffisamment compact pour s'adapter et fonctionner dans seulement 256 Ko d'espace de code et 16 Ko de RAM.

>> MicroPython vise à être aussi compatible que possible avec Python normal pour vous permettre de transférer facilement du code du bureau vers un microcontrôleur ou un système embarqué.

> **scikit-multiflow :**
>>Apprentissage incrémental : Les modèles d'apprentissage par flux sont créés progressivement et mis à jour en permanence. Ils conviennent aux applications Big Data où la réponse en temps réel est vitale.

>>Apprentissage adaptatif : Les changements dans la distribution des données nuisent à l'apprentissage. Les méthodes adaptatives sont spécifiquement conçues pour être robustes aux changements de dérive de concept dans des environnements dynamiques.

>>Efficace sur le plan des ressources : Les techniques de streaming gèrent efficacement les ressources telles que la mémoire et le temps de traitement étant donné la nature illimitée des flux de données.

>>Facile à utiliser : scikit-multiflow est conçu pour les utilisateurs de tout niveau d'expérience. Les expériences sont faciles à concevoir, à configurer et à exécuter. Les méthodes existantes sont faciles à modifier et à étendre.

>>Outils d'apprentissage en streaming : Dans son état actuel, scikit-multiflow contient des générateurs de données, des méthodes d'apprentissage de flux multi-sorties/multi-cibles, des méthodes de détection de changement, des méthodes d'évaluation, etc.

> **tinyML :** 
>> L'apprentissage automatique minuscule (**tinyML**) est généralement défini comme un domaine en croissance rapide des technologies et des applications d'apprentissage automatique, y compris le matériel (circuits intégrés dédiés), les algorithmes et les logiciels capables d'effectuer des analyses de données de capteurs sur l'appareil (vision, audio, IMU, biomédical, etc.) à une puissance extrêmement faible, généralement dans la plage des mW et moins, et permettant ainsi une variété de cas d'utilisation toujours actifs et ciblant les appareils fonctionnant sur batterie.

>> Le premier sommet tinyML en mars 2019 a montré un très vif intérêt de la part de la communauté avec la participation active d'experts seniors de 90 entreprises. Il a révélé que : 

>>> (i) le petit matériel capable d'apprendre par machine devient "assez bon" pour de nombreuses applications commerciales et de nouvelles architectures (par exemple, le calcul en mémoire) sont à l'horizon ; 
 
>>> (ii) des progrès significatifs sur les algorithmes, les réseaux et les modèles jusqu'à 100 Ko et moins ; 

>>> (iii) les applications initiales à faible puissance dans l'espace de la vision et de l'audio. Il y a une dynamique croissante démontrée par le progrès technique et le développement de l'écosystème.

>> La communauté tinyML continue de croître grâce à de multiples événements de haute qualité  <br>
>> – en personne et en ligne  <br>
>> – tout au long de l'année,  <br>
>> notamment le sommet tinyML, tinyML EMEA, tinyML Asia, tinyML Talks et les meetups tinyML.

# Résultats de nos RECHERCHES

## 1. Concepts et notions

La détection d'anomalies est une branche du forage de données qui s'occupe de l'identification des enregistrements atypiques ou des occurrences rares dans les
données (Tan et al., 2006). En d'autres termes, la détection d'anomalies consiste à trouver les objets qui sont différents ou inconsistants par rapport à la majorité des objets d 'un jeu de données. Dans la littérature, les objets atypiques détectés sont dits anomalies, et sont aussi appelés, selon le contexte d'application, exceptions, surprises ou outliers (Aggarwal, 2017).

Initialement, la détection d'anomalies s'est développée dans les données à vecteur de caractéristiques. Formellement, la première définition d'anomalie revient à Hawkins en 1980 : "Une anomalie est une observation qui diffère tellement d'autres observations au point d'éveiller des soupçons qu'elle soit générée par un mécanisme différent" (Hawkins, 1980). 
Étant donnée la force d'expressivité des graphes et leur capacité à représenter des relations complexes entre les entités du monde réel, la notion d'anomalie s'est généralisée au cas des données représentées par des graphes. En effet, "dans un graphe, une anomalie peut être définie par les objets qui sont rares et qui diffèrent significativement de la majorité des objets de référence ." (Akoglu et al. , 2015). Ici , un objet de référence est un objet qui se caractérise par un comportement ou par un état normal attendu.

La définition d'anomalie prend plus de sens lorsqu'elle est reliée à un contexte ou à une application bien spécifique. Particulièrement , la détection d'anomalies a été beaucoup appliquée pour dans la détection de fraude. Par ailleurs, la détection d'anomalies s'emploie aussi dans la détection des intrusions dans les réseaux d'ordinateurs. 

## 2. Détection d'anomalies dans les données vectorielles

La technique de détection d'anomalies dans les données vectorielles peuvent être catégorisées en trois types : méthodes à base de statistiques, méthodes à base de distance et méthodes à base de densité. 

### 2.1 Méthodes à base de statistiques

Les approches à base de statistiques consistent à élaborer des modèles statistiques probabilistes flexibles qui représentent la distribution des jeux de données testés comme les modèles gaussiens (Yamanishi et al., 2004) et les modèles de régression (Aggarwal, 2005) , (Li et Han , 2007). Le degré d'anomalie d'un objet particulier est évalué par rapport à sa conformité au modèle établi. Particulièrement, dans (Yamanishi et al., 2004), un modèle de mélange gaussien est proposé pour représenter la distribution des données testées. Chaque objet reçoit un score d'anomalie qui caractérise sa déviation au modèle. Un score élevé dénote une forte probabilité que l'objet en question soit une anomalie.

### 2.2 Méthodes à base de distance

Les méthodes à base de distance consistent à calculer la disparité entre les objets d'un ensemble de données. Pour mesurer l'hétérogénéité des objets, plusieurs métriques peuvent être employées comme la distance euclidienne et la distance de Manhattan. Un objet est considéré une anomalie s'il est remarquablement distant de la majorité d'objets.
Spécifiquement les techniques à base de distance comme k plus proches voisins (Ramaswamy et al. , 2000) et KNN-pondéré (Angiulli et Pizzuti, 2002) assignent un score d'anomalie à chaque objet en se basant sur ses k plus proches voisins. De cette manière, étant distants , les anomalies ( outliers) reçoivent des scores élevés et les objets normaux ( inliers) reçoivent des scores faibles. 
Les anomalies sont discernées en triant les scores dans un ordre ascendant et en sélectionnant les observations ayant les scores les moins élevés.

### 2.3 Méthodes à base de densité

Les méthodes à base de densité mesurent le degré d'anomalie d'un objet en considérant la densité locale de son voisinage. Spécifiquement, l'exemple de calcul du score d'anomalie LOF (Local Outlier Factor) (Breunig et al., 2000). Le fondement de LOF a été inspiré de la méthode de partitionnement à base de densité DBSCAN qui identifie à la fois les communautés et les outliers (Ester et al., 1996). 
Dans LOF, la densité locale de chaque objet se calcule en respect de ses k plus proches voisins. L'ensemble des distances d'un objet particulier à ses k plus proches voisins sont utilisées dans le calcul de sa densité locale. Les densités locales de tous les objets sont, ensuite, évaluées pour déterminer les régions de densité similaires et les objets outliers qui détiennent des densités locales remarquablement faibles par rapport à leurs voisinages. 
Les techniques présentées dans cette section ne représentent pas une liste exhaustive des méthodes de détection d'anomalies dans les données vectorielles. 

## 3. Détection d'anomalies dans les graphes

Un grand intérêt a été porté à l'élaboration de techniques qui traite les anomalies dans les graphes, et ce vu leur expressivité et leur capacité à représenter la complexité d'interaction du monde réel (Akoglu et al., 2015). Les techniques présentées dans la suite de cette section, discernent les anomalies à partir de la structure topologique d 'un graphe monodimensionnel.
La topologie d'un réseau est porteuse d'informations implicites décisives pour repérer les anomalies. À titre d'exemple, dans les réseaux sociaux, plusieurs types d'anomalies peuvent avoir lieu (Savage et al., 2014). Particulièrement, les spammeurs sont incapables de cacher un certain type de métadonnées telles que leurs patrons d'interactions, c'est-à-dire, les liens qu 'ils établissent et qui sont, entre autres, révélateurs de leur comportement irrégulier. Pour identifier ces acteurs malveillants du réseau, l'utilisation unique des données est insuffisante et il est nécessaire de considérer la structure topologique du réseau. Dans (Fathaliani et Bouguessa, 2015), un score d'anomalie qui évalue la proportion des liens émis et des liens reçus a été proposé pour identifier ce type d'utilisateurs malintentionnés.
À cela s'ajoute, dans les graphes, la détection de communautés qui a été appliquée pour la détection des intrusions dans les réseaux (Ding et al. , 2012). En effet, la détection de communautés (appelée aussi partitionnement) compte parmi les problématiques les plus étudiées dans les graphes. Explicitement, la détection de communautés consiste à découvrir la structure sous-jacente d'un réseau à savoir l'identification des nœuds fortement connectés entre eux (c'est-à-dire, les clusters). Dans ce contexte, les intrusions sont les nœuds présents dans une communauté, mais qui n'y appartiennent pas réellement. D'autres techniques de détection de communautés se sont concentrées sur l'identification des nœuds superflus ( outliers) qui sont marginalement connectés aux communautés. Ces nœuds forment un bruit dans le réseau et leur élimination peut, potentiellement , améliorer les résultats d'analyse . Un nombre d'algorithmes a été proposé à ce suj et, à savoir SCAN (Xu et al., 2007), gSkeletonClu (Huang et al., 2013) et SHRINK (Huang et al., 2010). L'algorithme SCAN détecte les communautés et les nœuds qui y sont marginalement connectés en se basant sur une métrique de similarité structurale qui se calcule entre les nœuds du graphe. Cette métrique tient compte de la connectivité entre les nœuds, et ce en examinant leurs voisinages. Plus les nœuds partagent des voisins, plus la valeur de la métrique de similarité est élevée. Pour déterminer les communautés et les outliers, un seuil minimal de similarité doit être fixé. De cette manière, les nœuds qui ont plusieurs voisins en commun se groupent dans une même communauté. Par ailleurs, les nœuds superflus sont les nœuds qui ne se sont pas affectés à des communautés et qui ont des valeurs de similarité faibles.
L'algorithme SCAN nécessite un paramétrage pour pouvoir identifier les communautés et les nœuds superflus. Pour éviter cette intervention supervisée, l'algorithme gSkeletonClu a été proposé. Il convient de préciser que, tout comme SCAN, l'algorithme gSkeletonClu est un algorithme de partitionnement à base de densité qui détermine aussi bien les communautés tous les outliers selon une métrique de similarité. Toutefois, l'algorithme gSkeletonClu permet une sélection automatique de la valeur du seuil minimal, et ce en maximisant une mesure de validité comme la modularité. La modularité est une métrique fréquemment utilisée pour mesurer la qualité de partitionnement d'un graphe. Quelques algorithmes comme gSkeletonClu utilisent cette métrique comme une fonction objective pour optimiser le partitionnement d'un graphe. Ici, il est utile de noter que , dans gSkeletonClu, l'extraction des communautés et des outliers peut se faire aussi avec un seuil spécifié à l'avance par l'utilisateur.
Dans la littérature, nous trouvons, également, SHRINK qui est un algorithme de partitionnement hiérarchique. Cet algorithme identifie les communautés et les
outliers sans la nécessité de paramétrage en reposant sur le principe de réduction. Plus précisément, l'algorithme commence par identifier les pairs de nœuds denses, c.-à-d. les nœuds dont la similarité structurale est maximale par rapport à leurs voisinages. Ensuite, une fusion se fait entre les pairs de nœuds denses identifiés pour construire itérativement des microcommunautés. De ce fait, une microcommunauté peut être un nœud isolé ou un sous-graphe d 'un ou plusieurs pairs de nœuds denses connectés. Au fur et à mesure des itérations, un arbre hiérarchique d'emboîtement des microcommunautés se construit.

## 4. Isolation-based

le principe de l'isolation-based approche consiste à isoler les observations anormales de l'ensemble de données. Données d'anomalies
sont censés être très différents des normaux. Ils sont également censés représentent une très petite proportion de l'ensemble des données. Ainsi, ils sont susceptibles d'être rapidement isolé. Certaines méthodes sont basées sur l'isolement. Les méthodes basées sur l'isolement sont différentes des autres statistiques, clustering ou plus proches approches voisines car elles ne calculent pas une distance ou une densité à partir de l'ensemble de données. Par conséquent, ils ont une complexité moindre et sont plus évolutifs. Ils ne souffrent pas du problème de CPU, de mémoire ou de consommation de temps. Ainsi, 
les méthodes basées sur l'isolation sont adaptées au contexte du flux de données.

## 5. Deep learning 

il représente une classe d’algorithmes d’apprentissage automatique supervisé ou non supervisé basés sur l’utilisation de plusieurs couches d’unité de traitement non linéaire. Parmi ces méthodes on cite les auto-encoders (AE) et One-Class Neural Networks (OC-NN) (Chalapathy et Chawla (2019)).

## 6. Autres techniques

Basées sur les machines à vecteurs de support (Schölkopf et al. (2000)), les réseaux de neurones (Hodge et Austin
(2004)), les méthodes adaptées aux grandes dimensions par construction de sous-espaces ou par réduction de dimension (Aggarwal (2017)).


## 7. Récapitulatif de la Classification des différentes techniques de détection d’anomalies

 <!-- ![Classification des différentes techniques de détection d’anomalies](/images/recapt-detect-anom.png) -->

<p align="center">       
        <img src="images/recapt-detect-anom.png" alt="Classification des différentes techniques de détection d’anomalies"> 
</p>

## 8.Classification of data stream anomaly detection methods

<p align="center">       
        <img src="images/recap-data-stream-anom.png" alt="Classification des différentes techniques de détection d’anomalies"> 
</p>



## Méthodes  expérimentales de détection d’anomalies très utilisées dans la littérature

Méthodes se basant à la fois sur le type de jeux de données (flux, séries temporelles, graphes, etc.), le domaine d’application et l’approche considérée (statistique, classification, clustering, etc.). Trois algorithmes : 
- **LOF**, Local Outlier Factor (LOF) est une méthode phare de la détection d’anomalies locales basée sur la densité de l’observation en question par rapport à la densité de ses plus proches voisins. Proposée par Breunig et al. (2000), LOF est une méthode non supervisée qui donne un score représentant le degré d’aberrance de l’observation. Les observations dont le degré d’aberrance est largement supérieur à 1 sont considérées comme anomalies. 
- **OC-SVM**, One-Class Support Vector Machine (OC-SVM) est une méthode de détection d’anomalies qui applique des algorithmes de SVM au problème de One class classification (OCC) proposée par Schölkopf et al. (2000, 2001). Le séparateur à vaste marge (SVM) appelé aussi machine à vecteurs de support est très utilisé pour l’apprentissage automatique du fait de sa puissance et de sa polyvalence (classification linéaire, non-linéaire, régression). OCC est une approche de classification semi-supervisée qui consiste à repérer toutes les observations appartenant à une classe précise connue pendant l’apprentissage, dans tout le jeu de données. L’idée clé de cette méthode est de trouver un hyperplan dans un espace de grande dimension qui sépare les anomalies des données normales.
- **Isolation Forest**, IForest est une méthode basée sur les arbres de décision et les forêts aléatoires (Liu et al. (2008, 2012)). Elle utilise l’isolation d’observations à partir de la construction de plusieurs arbres aléatoires. Quand une forêt d’arbres aléatoires et indépendants produit collectivement un chemin d’accès court pour atteindre une observation depuis la racine, celle-ci a une forte probabilité d’être une anomalie. Le nombre d’arbres utilisés est donc un important paramètre pour IForest. Le seuil de la détection est aussi un paramètre clé, il est donné par le score calculé pour chaque observation relativement aux autres observations.

Nous privilégierons une approche basée sur l'isolement : Isolation Forest.

À ce propos vérifions si ces programmes en langage C++ ne traitent pas déjà la question

  1. [ISOTREE](https://github.com/antaresatlantide/anomalie-detection/blob/main/isotree.md)
  2. [LIBISOLATIONFOREST](https://github.com/antaresatlantide/anomalie-detection/blob/main/LibIsolationForest.md)
  3. [RANGER](https://github.com/antaresatlantide/anomalie-detection/blob/main/ranger.md)
  4. [Machine Learning From Scratch with C++](https://github.com/antaresatlantide/anomalie-detection/blob/main/MLfromcrashcpp.md)
     - 4.1 [K-Means](https://github.com/antaresatlantide/anomalie-detection/blob/main/MLfromcrashcpp.md)
     - 4.2 [K-NN](https://github.com/antaresatlantide/anomalie-detection/blob/main/MLfromcrashcpp.md)
     - 4.3 [REGRESSIONTREES](https://github.com/antaresatlantide/anomalie-detection/blob/main/MLfromcrashcpp.md)
    
  
